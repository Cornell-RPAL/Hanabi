{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cornell.edu/nst45/.local/share/virtualenvs/Hanabi-6Eo08z-l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, LSTM, Flatten, BatchNormalization, TimeDistributed, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"x.npy\")\n",
    "y = np.load(\"y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203, 67, 3)\n",
      "(51, 67, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = keras.utils.normalize(x_train, axis = 1)\n",
    "x_test = keras.utils.normalize(x_test, axis = 1)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 67\n",
    "_dropout = 0.5\n",
    "_activation='relu'\n",
    "_optimizer='adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 1.6883 - acc: 0.3202\n",
      "Epoch 2/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.1915 - acc: 0.4975\n",
      "Epoch 3/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.0454 - acc: 0.5419\n",
      "Epoch 4/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.0239 - acc: 0.5813\n",
      "Epoch 5/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.0407 - acc: 0.6108\n",
      "Epoch 6/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.9010 - acc: 0.5862\n",
      "Epoch 7/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.7915 - acc: 0.6502\n",
      "Epoch 8/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8488 - acc: 0.6404\n",
      "Epoch 9/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.7382 - acc: 0.7438\n",
      "Epoch 10/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8590 - acc: 0.6552\n",
      "Epoch 11/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8803 - acc: 0.6700\n",
      "Epoch 12/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.7519 - acc: 0.7094\n",
      "Epoch 13/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6237 - acc: 0.7438\n",
      "Epoch 14/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6502 - acc: 0.7340\n",
      "Epoch 15/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.7786 - acc: 0.7143\n",
      "Epoch 16/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8456 - acc: 0.7340\n",
      "Epoch 17/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6065 - acc: 0.7438\n",
      "Epoch 18/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.7549 - acc: 0.6995\n",
      "Epoch 19/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5176 - acc: 0.7833\n",
      "Epoch 20/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6964 - acc: 0.7389\n",
      "Epoch 21/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5712 - acc: 0.7882\n",
      "Epoch 22/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4537 - acc: 0.7980\n",
      "Epoch 23/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8119 - acc: 0.6552\n",
      "Epoch 24/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.7867 - acc: 0.6946\n",
      "Epoch 25/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4423 - acc: 0.7931\n",
      "Epoch 26/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.7842 - acc: 0.7192\n",
      "Epoch 27/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8159 - acc: 0.7143\n",
      "Epoch 28/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5915 - acc: 0.7882\n",
      "Epoch 29/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5353 - acc: 0.7389\n",
      "Epoch 30/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4849 - acc: 0.8030\n",
      "Epoch 31/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6157 - acc: 0.7537\n",
      "Epoch 32/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5361 - acc: 0.7783\n",
      "Epoch 33/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4976 - acc: 0.7783\n",
      "Epoch 34/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5417 - acc: 0.7833\n",
      "Epoch 35/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5338 - acc: 0.7635\n",
      "Epoch 36/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4545 - acc: 0.7980\n",
      "Epoch 37/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5814 - acc: 0.8177\n",
      "Epoch 38/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4454 - acc: 0.8128\n",
      "Epoch 39/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5717 - acc: 0.7734\n",
      "Epoch 40/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5573 - acc: 0.8030\n",
      "Epoch 41/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6323 - acc: 0.7734\n",
      "Epoch 42/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5810 - acc: 0.7734\n",
      "Epoch 43/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5449 - acc: 0.7537\n",
      "Epoch 44/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5150 - acc: 0.7833\n",
      "Epoch 45/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4062 - acc: 0.7980\n",
      "Epoch 46/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5454 - acc: 0.7980\n",
      "Epoch 47/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3905 - acc: 0.8030\n",
      "Epoch 48/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3964 - acc: 0.8177\n",
      "Epoch 49/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4189 - acc: 0.8473\n",
      "Epoch 50/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5101 - acc: 0.7685\n",
      "Epoch 51/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4931 - acc: 0.8128\n",
      "Epoch 52/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4360 - acc: 0.8079\n",
      "Epoch 53/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4078 - acc: 0.8424\n",
      "Epoch 54/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3880 - acc: 0.8571\n",
      "Epoch 55/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4431 - acc: 0.8177\n",
      "Epoch 56/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3769 - acc: 0.8177\n",
      "Epoch 57/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3961 - acc: 0.8177\n",
      "Epoch 58/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3401 - acc: 0.8670\n",
      "Epoch 59/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3400 - acc: 0.8670\n",
      "Epoch 60/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4031 - acc: 0.8325\n",
      "Epoch 61/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3458 - acc: 0.8621\n",
      "Epoch 62/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3796 - acc: 0.8227\n",
      "Epoch 63/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3733 - acc: 0.8374\n",
      "Epoch 64/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4893 - acc: 0.8128\n",
      "Epoch 65/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4596 - acc: 0.7980\n",
      "Epoch 66/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3477 - acc: 0.8424\n",
      "Epoch 67/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3792 - acc: 0.8424\n",
      "Epoch 68/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3779 - acc: 0.8128\n",
      "Epoch 69/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3980 - acc: 0.8325\n",
      "Epoch 70/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2998 - acc: 0.8670\n",
      "Epoch 71/350\n",
      "203/203 [==============================] - 1s 3ms/step - loss: 0.5205 - acc: 0.7882\n",
      "Epoch 72/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3791 - acc: 0.8325\n",
      "Epoch 73/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3590 - acc: 0.8571\n",
      "Epoch 74/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3412 - acc: 0.8571\n",
      "Epoch 75/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.7704 - acc: 0.7586\n",
      "Epoch 76/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.7803 - acc: 0.7586\n",
      "Epoch 77/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3938 - acc: 0.8522\n",
      "Epoch 78/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2915 - acc: 0.8522\n",
      "Epoch 79/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3266 - acc: 0.8473\n",
      "Epoch 80/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3269 - acc: 0.8621\n",
      "Epoch 81/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4027 - acc: 0.8079\n",
      "Epoch 82/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3976 - acc: 0.8276\n",
      "Epoch 83/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4191 - acc: 0.8079\n",
      "Epoch 84/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3821 - acc: 0.8374\n",
      "Epoch 85/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3404 - acc: 0.8818\n",
      "Epoch 86/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2844 - acc: 0.8670\n",
      "Epoch 87/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3831 - acc: 0.8177\n",
      "Epoch 88/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3275 - acc: 0.8571\n",
      "Epoch 89/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2115 - acc: 0.8966\n",
      "Epoch 90/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2562 - acc: 0.8818\n",
      "Epoch 91/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4577 - acc: 0.8325\n",
      "Epoch 92/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3152 - acc: 0.8571\n",
      "Epoch 93/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6496 - acc: 0.7685\n",
      "Epoch 94/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4858 - acc: 0.8128\n",
      "Epoch 95/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4376 - acc: 0.8325\n",
      "Epoch 96/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4198 - acc: 0.8177\n",
      "Epoch 97/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3107 - acc: 0.8621\n",
      "Epoch 98/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3672 - acc: 0.8670\n",
      "Epoch 99/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4058 - acc: 0.8473\n",
      "Epoch 100/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3558 - acc: 0.8473\n",
      "Epoch 101/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3115 - acc: 0.8818\n",
      "Epoch 102/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3976 - acc: 0.8227\n",
      "Epoch 103/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4232 - acc: 0.8621\n",
      "Epoch 104/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3947 - acc: 0.8473\n",
      "Epoch 105/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3527 - acc: 0.8522\n",
      "Epoch 106/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3661 - acc: 0.8424\n",
      "Epoch 107/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3471 - acc: 0.8325\n",
      "Epoch 108/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3297 - acc: 0.8473\n",
      "Epoch 109/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3375 - acc: 0.8670\n",
      "Epoch 110/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3262 - acc: 0.8670\n",
      "Epoch 111/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3110 - acc: 0.8818\n",
      "Epoch 112/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3846 - acc: 0.8276\n",
      "Epoch 113/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3619 - acc: 0.8177\n",
      "Epoch 114/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4418 - acc: 0.8079\n",
      "Epoch 115/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4029 - acc: 0.8325\n",
      "Epoch 116/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3288 - acc: 0.8473\n",
      "Epoch 117/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2944 - acc: 0.8571\n",
      "Epoch 118/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2870 - acc: 0.8818\n",
      "Epoch 119/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3257 - acc: 0.8768\n",
      "Epoch 120/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3469 - acc: 0.8374\n",
      "Epoch 121/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2216 - acc: 0.8916\n",
      "Epoch 122/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2655 - acc: 0.8768\n",
      "Epoch 123/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2105 - acc: 0.9163\n",
      "Epoch 124/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2321 - acc: 0.9113\n",
      "Epoch 125/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2584 - acc: 0.8916\n",
      "Epoch 126/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2451 - acc: 0.9261\n",
      "Epoch 127/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2602 - acc: 0.9015\n",
      "Epoch 128/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2881 - acc: 0.8818\n",
      "Epoch 129/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3950 - acc: 0.8473\n",
      "Epoch 130/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3318 - acc: 0.8670\n",
      "Epoch 131/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1969 - acc: 0.9163\n",
      "Epoch 132/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3063 - acc: 0.8768\n",
      "Epoch 133/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2148 - acc: 0.9163\n",
      "Epoch 134/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2556 - acc: 0.8916\n",
      "Epoch 135/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1532 - acc: 0.9310\n",
      "Epoch 136/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1997 - acc: 0.9113\n",
      "Epoch 137/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2385 - acc: 0.9015\n",
      "Epoch 138/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2866 - acc: 0.8621\n",
      "Epoch 139/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4209 - acc: 0.8374\n",
      "Epoch 140/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3475 - acc: 0.8818\n",
      "Epoch 141/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2580 - acc: 0.9015\n",
      "Epoch 142/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2286 - acc: 0.9261\n",
      "Epoch 143/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3032 - acc: 0.8571\n",
      "Epoch 144/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2852 - acc: 0.8818\n",
      "Epoch 145/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2530 - acc: 0.8966\n",
      "Epoch 146/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2973 - acc: 0.8621\n",
      "Epoch 147/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2537 - acc: 0.9015\n",
      "Epoch 148/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2340 - acc: 0.9113\n",
      "Epoch 149/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1965 - acc: 0.9310\n",
      "Epoch 150/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1952 - acc: 0.9113\n",
      "Epoch 151/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2260 - acc: 0.9163\n",
      "Epoch 152/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3036 - acc: 0.8768\n",
      "Epoch 153/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3963 - acc: 0.8719\n",
      "Epoch 154/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3435 - acc: 0.8867\n",
      "Epoch 155/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1987 - acc: 0.9261\n",
      "Epoch 156/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3007 - acc: 0.8621\n",
      "Epoch 157/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2359 - acc: 0.9113\n",
      "Epoch 158/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1930 - acc: 0.9064\n",
      "Epoch 159/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2334 - acc: 0.8966\n",
      "Epoch 160/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2175 - acc: 0.9212\n",
      "Epoch 161/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1850 - acc: 0.9261\n",
      "Epoch 162/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1583 - acc: 0.9458\n",
      "Epoch 163/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2485 - acc: 0.9015\n",
      "Epoch 164/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3106 - acc: 0.8966\n",
      "Epoch 165/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2774 - acc: 0.8966\n",
      "Epoch 166/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2566 - acc: 0.8818\n",
      "Epoch 167/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2889 - acc: 0.8670\n",
      "Epoch 168/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2417 - acc: 0.9015\n",
      "Epoch 169/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2338 - acc: 0.9015\n",
      "Epoch 170/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3402 - acc: 0.8424\n",
      "Epoch 171/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2582 - acc: 0.9015\n",
      "Epoch 172/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2959 - acc: 0.8670\n",
      "Epoch 173/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3257 - acc: 0.8768\n",
      "Epoch 174/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1954 - acc: 0.9261\n",
      "Epoch 175/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2309 - acc: 0.8916\n",
      "Epoch 176/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2555 - acc: 0.8867\n",
      "Epoch 177/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2799 - acc: 0.8818\n",
      "Epoch 178/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1868 - acc: 0.9163\n",
      "Epoch 179/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1838 - acc: 0.9310\n",
      "Epoch 180/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1485 - acc: 0.9458\n",
      "Epoch 181/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2114 - acc: 0.9015\n",
      "Epoch 182/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1623 - acc: 0.9212\n",
      "Epoch 183/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1797 - acc: 0.9163\n",
      "Epoch 184/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1400 - acc: 0.9458\n",
      "Epoch 185/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1714 - acc: 0.9310\n",
      "Epoch 186/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1558 - acc: 0.9310\n",
      "Epoch 187/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1441 - acc: 0.9360\n",
      "Epoch 188/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1347 - acc: 0.9655\n",
      "Epoch 189/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1123 - acc: 0.9655\n",
      "Epoch 190/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1043 - acc: 0.9507\n",
      "Epoch 191/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1408 - acc: 0.9409\n",
      "Epoch 192/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1097 - acc: 0.9704\n",
      "Epoch 193/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1117 - acc: 0.9557\n",
      "Epoch 194/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1324 - acc: 0.9458\n",
      "Epoch 195/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1095 - acc: 0.9557\n",
      "Epoch 196/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1714 - acc: 0.9212\n",
      "Epoch 197/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1185 - acc: 0.9606\n",
      "Epoch 198/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1544 - acc: 0.9507\n",
      "Epoch 199/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0989 - acc: 0.9557\n",
      "Epoch 200/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1395 - acc: 0.9458\n",
      "Epoch 201/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1099 - acc: 0.9704\n",
      "Epoch 202/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0979 - acc: 0.9655\n",
      "Epoch 203/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0667 - acc: 0.9852\n",
      "Epoch 204/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1327 - acc: 0.9458\n",
      "Epoch 205/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2174 - acc: 0.9261\n",
      "Epoch 206/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1295 - acc: 0.9507\n",
      "Epoch 207/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1103 - acc: 0.9507\n",
      "Epoch 208/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1024 - acc: 0.9606\n",
      "Epoch 209/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1388 - acc: 0.9261\n",
      "Epoch 210/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0999 - acc: 0.9557\n",
      "Epoch 211/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1634 - acc: 0.9212\n",
      "Epoch 212/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1706 - acc: 0.9212\n",
      "Epoch 213/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1384 - acc: 0.9507\n",
      "Epoch 214/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1307 - acc: 0.9409\n",
      "Epoch 215/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1536 - acc: 0.9360\n",
      "Epoch 216/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3448 - acc: 0.8719\n",
      "Epoch 217/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1747 - acc: 0.9409\n",
      "Epoch 218/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1915 - acc: 0.9360\n",
      "Epoch 219/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1567 - acc: 0.9360\n",
      "Epoch 220/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1360 - acc: 0.9409\n",
      "Epoch 221/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1538 - acc: 0.9458\n",
      "Epoch 222/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1329 - acc: 0.9360\n",
      "Epoch 223/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0988 - acc: 0.9655\n",
      "Epoch 224/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1404 - acc: 0.9409\n",
      "Epoch 225/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1944 - acc: 0.9409\n",
      "Epoch 226/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1380 - acc: 0.9557\n",
      "Epoch 227/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1405 - acc: 0.9310\n",
      "Epoch 228/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1302 - acc: 0.9458\n",
      "Epoch 229/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0901 - acc: 0.9704\n",
      "Epoch 230/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1059 - acc: 0.9458\n",
      "Epoch 231/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0621 - acc: 0.9852\n",
      "Epoch 232/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1036 - acc: 0.9507\n",
      "Epoch 233/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1182 - acc: 0.9409\n",
      "Epoch 234/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1201 - acc: 0.9557\n",
      "Epoch 235/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1003 - acc: 0.9606\n",
      "Epoch 236/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1216 - acc: 0.9557\n",
      "Epoch 237/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1610 - acc: 0.9360\n",
      "Epoch 238/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0927 - acc: 0.9557\n",
      "Epoch 239/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1880 - acc: 0.9310\n",
      "Epoch 240/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2209 - acc: 0.9310\n",
      "Epoch 241/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1644 - acc: 0.9310\n",
      "Epoch 242/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2364 - acc: 0.9113\n",
      "Epoch 243/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4575 - acc: 0.8719\n",
      "Epoch 244/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3271 - acc: 0.8571\n",
      "Epoch 245/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2765 - acc: 0.8966\n",
      "Epoch 246/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2459 - acc: 0.9113\n",
      "Epoch 247/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2130 - acc: 0.9064\n",
      "Epoch 248/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1975 - acc: 0.9113\n",
      "Epoch 249/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2020 - acc: 0.9212\n",
      "Epoch 250/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1350 - acc: 0.9458\n",
      "Epoch 251/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2115 - acc: 0.9360\n",
      "Epoch 252/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2124 - acc: 0.9015\n",
      "Epoch 253/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1460 - acc: 0.9409\n",
      "Epoch 254/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1632 - acc: 0.9163\n",
      "Epoch 255/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1631 - acc: 0.9409\n",
      "Epoch 256/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1541 - acc: 0.9261\n",
      "Epoch 257/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0892 - acc: 0.9507\n",
      "Epoch 258/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1282 - acc: 0.9557\n",
      "Epoch 259/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1074 - acc: 0.9557\n",
      "Epoch 260/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1385 - acc: 0.9458\n",
      "Epoch 261/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3054 - acc: 0.8966\n",
      "Epoch 262/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1720 - acc: 0.9310\n",
      "Epoch 263/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1797 - acc: 0.9310\n",
      "Epoch 264/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1160 - acc: 0.9704\n",
      "Epoch 265/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1293 - acc: 0.9606\n",
      "Epoch 266/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1620 - acc: 0.9310\n",
      "Epoch 267/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1012 - acc: 0.9557\n",
      "Epoch 268/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0907 - acc: 0.9606\n",
      "Epoch 269/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1051 - acc: 0.9557\n",
      "Epoch 270/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1060 - acc: 0.9507\n",
      "Epoch 271/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0882 - acc: 0.9704\n",
      "Epoch 272/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0874 - acc: 0.9606\n",
      "Epoch 273/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1216 - acc: 0.9507\n",
      "Epoch 274/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1673 - acc: 0.9409\n",
      "Epoch 275/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1638 - acc: 0.9163\n",
      "Epoch 276/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1210 - acc: 0.9507\n",
      "Epoch 277/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1993 - acc: 0.9261\n",
      "Epoch 278/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2155 - acc: 0.9360\n",
      "Epoch 279/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1301 - acc: 0.9557\n",
      "Epoch 280/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1292 - acc: 0.9360\n",
      "Epoch 281/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2188 - acc: 0.9015\n",
      "Epoch 282/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1754 - acc: 0.9064\n",
      "Epoch 283/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2154 - acc: 0.9163\n",
      "Epoch 284/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2250 - acc: 0.9360\n",
      "Epoch 285/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1557 - acc: 0.9507\n",
      "Epoch 286/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1641 - acc: 0.9409\n",
      "Epoch 287/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1657 - acc: 0.9360\n",
      "Epoch 288/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1317 - acc: 0.9557\n",
      "Epoch 289/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2277 - acc: 0.9360\n",
      "Epoch 290/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3114 - acc: 0.8670\n",
      "Epoch 291/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1399 - acc: 0.9557\n",
      "Epoch 292/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1951 - acc: 0.9015\n",
      "Epoch 293/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3670 - acc: 0.8571\n",
      "Epoch 294/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2753 - acc: 0.9064\n",
      "Epoch 295/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3146 - acc: 0.8867\n",
      "Epoch 296/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3256 - acc: 0.8768\n",
      "Epoch 297/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3605 - acc: 0.8571\n",
      "Epoch 298/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2888 - acc: 0.8867\n",
      "Epoch 299/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3648 - acc: 0.8768\n",
      "Epoch 300/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6381 - acc: 0.7783\n",
      "Epoch 301/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3200 - acc: 0.8522\n",
      "Epoch 302/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2504 - acc: 0.8916\n",
      "Epoch 303/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3224 - acc: 0.8473\n",
      "Epoch 304/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2740 - acc: 0.8818\n",
      "Epoch 305/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1864 - acc: 0.8966\n",
      "Epoch 306/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1988 - acc: 0.9360\n",
      "Epoch 307/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2767 - acc: 0.9015\n",
      "Epoch 308/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2148 - acc: 0.9212\n",
      "Epoch 309/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1723 - acc: 0.9212\n",
      "Epoch 310/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2098 - acc: 0.9212\n",
      "Epoch 311/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2196 - acc: 0.9310\n",
      "Epoch 312/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1432 - acc: 0.9409\n",
      "Epoch 313/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1604 - acc: 0.9261\n",
      "Epoch 314/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1074 - acc: 0.9458\n",
      "Epoch 315/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1237 - acc: 0.9458\n",
      "Epoch 316/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1185 - acc: 0.9557\n",
      "Epoch 317/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1813 - acc: 0.9163\n",
      "Epoch 318/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1379 - acc: 0.9507\n",
      "Epoch 319/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1491 - acc: 0.9507\n",
      "Epoch 320/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1700 - acc: 0.9261\n",
      "Epoch 321/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0934 - acc: 0.9557\n",
      "Epoch 322/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1146 - acc: 0.9458\n",
      "Epoch 323/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0847 - acc: 0.9655\n",
      "Epoch 324/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1038 - acc: 0.9606\n",
      "Epoch 325/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0886 - acc: 0.9655\n",
      "Epoch 326/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1289 - acc: 0.9507\n",
      "Epoch 327/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1057 - acc: 0.9704\n",
      "Epoch 328/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1620 - acc: 0.9360\n",
      "Epoch 329/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1326 - acc: 0.9507\n",
      "Epoch 330/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1483 - acc: 0.9360\n",
      "Epoch 331/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1073 - acc: 0.9557\n",
      "Epoch 332/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1010 - acc: 0.9704\n",
      "Epoch 333/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1341 - acc: 0.9409\n",
      "Epoch 334/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1299 - acc: 0.9409\n",
      "Epoch 335/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.0855 - acc: 0.9754\n",
      "Epoch 336/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1094 - acc: 0.9606\n",
      "Epoch 337/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1414 - acc: 0.9458\n",
      "Epoch 338/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1441 - acc: 0.9557\n",
      "Epoch 339/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1677 - acc: 0.9507\n",
      "Epoch 340/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2166 - acc: 0.8916\n",
      "Epoch 341/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3480 - acc: 0.8571\n",
      "Epoch 342/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5172 - acc: 0.8079\n",
      "Epoch 343/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4677 - acc: 0.8424\n",
      "Epoch 344/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3318 - acc: 0.8719\n",
      "Epoch 345/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2163 - acc: 0.9015\n",
      "Epoch 346/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2742 - acc: 0.8966\n",
      "Epoch 347/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2563 - acc: 0.9064\n",
      "Epoch 348/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2793 - acc: 0.8719\n",
      "Epoch 349/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.2604 - acc: 0.8916\n",
      "Epoch 350/350\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.1554 - acc: 0.9507\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 67, 128)           67584     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 67, 128)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 67, 128)           512       \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 67, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 67, 128)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 67, 128)           512       \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 67, 64)            49408     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 67, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 67, 32)            12416     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 67, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 67, 32)            128       \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 67, 3)             99        \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 201)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 606       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 263,105\n",
      "Trainable params: 262,401\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "51/51 [==============================] - 2s 30ms/step\n",
      "Test score: 0.463\n",
      "Test accuracy: 0.784\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train.shape[1], x_train.shape[2]\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=input_shape, return_sequences = True))\n",
    "model.add(Dropout(0.5))                             \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape=input_shape, return_sequences = True))\n",
    "model.add(Dropout(0.5))                             \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(64, input_shape=input_shape, return_sequences = True))\n",
    "model.add(Dropout(0.5))                             \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(32, input_shape=input_shape, return_sequences = True))\n",
    "model.add(Dropout(0.5))                             \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(TimeDistributed(Dense(3))) \n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(y_train.shape[1], activation = tf.nn.softmax))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# model.add(TimeDistributed(Dense(128, input_shape = input_shape)))\n",
    "# myOptimizer = Adam(lr = 0.001) \n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation = tf.nn.relu))\n",
    "# model.add(Dropout(_dropout))\n",
    "# model.add(Dense(128, activation = tf.nn.relu))\n",
    "# model.add(Dropout(_dropout))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(64, activation = tf.nn.relu))\n",
    "# model.add(Dropout(_dropout))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(32, activation = tf.nn.relu))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(y_train.shape[1], activation = tf.nn.softmax))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "# # model.add(Dropout(0.5))\n",
    "# # model.add(Dense(128, activation='relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# # model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(optimizer='Adam',\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=350,batch_size=67)\n",
    "model.summary()\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer = _optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(TimeDistributed(Dense(X_vector_dim, activation=_activation), input_shape=input_shape))\n",
    "#     \n",
    "#     model.add(TimeDistributed(Dense(X_vector_dim*2, activation=_activation))) #(5, 80)\n",
    "#     model.add(Dropout(_dropout))\n",
    "#     model.add(TimeDistributed(Dense(X_vector_dim, activation=_activation))) #(5, 40)\n",
    "#     model.add(Dropout(_dropout))\n",
    "#     model.add(TimeDistributed(Dense(X_vector_dim/2, activation=_activation))) #(5, 20)\n",
    "#     model.add(Dropout(_dropout))\n",
    "#     model.add(TimeDistributed(Dense(X_vector_dim/4, activation=_activation))) #(5, 10)\n",
    "#     model.add(Dropout(_dropout))\n",
    "#     model.add(LSTM(X_vector_dim/4, dropout=_dropout, recurrent_dropout=_dropout))\n",
    "#     model.add(Dense(y_vector_dim,activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=_optimizer, metrics=['accuracy'])\n",
    "#     model.summary()\n",
    "\n",
    "# # Fit model\n",
    "# print('Training...')\n",
    "# model.fit(x_train, y_train,\n",
    "#          batch_size=batch_size,\n",
    "#          epochs=epochs,\n",
    "#          validation_data=(x_test, y_test))\n",
    "# # Evaluate Model and Predict Classes\n",
    "# print('Testing...')\n",
    "score, accuracy = model.evaluate(x_test, y_test,\n",
    "                                batch_size=67)\n",
    "\n",
    "print('Test score: {:.3}'.format(score))\n",
    "print('Test accuracy: {:.3}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('pointing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 2s 38ms/step\n",
      "Test score: 0.202\n",
      "Test accuracy: 0.961\n"
     ]
    }
   ],
   "source": [
    "model1 = load_model('pointing.h5')\n",
    "score, accuracy = model1.evaluate(x_test, y_test,\n",
    "                                batch_size=67)\n",
    "\n",
    "print('Test score: {:.3}'.format(score))\n",
    "print('Test accuracy: {:.3}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
